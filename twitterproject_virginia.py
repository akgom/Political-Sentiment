# -*- coding: utf-8 -*-
"""twitterproject_VIRGINIA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CCTy1TGZm0lsBDzzHJ_KkNmE4C75eP-4
"""

pip install git+https://github.com/tweepy/tweepy.git

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split

# Define the BERT-based sentiment analysis model
class TweetSentimentClassifier(nn.Module):
    def __init__(self, pretrained_model_name, num_classes):
        super(TweetSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = outputs['pooler_output']
        output = self.dropout(pooled_output)
        return self.fc(output)

def train_sentiment_model(dataset, pretrained_model_name='vinai/bertweet-base', num_classes=3, batch_size=16, num_epochs=3):
    # Tokenize and encode text data using the BERT tokenizer
    tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)
    encoded_tweets = tokenizer(dataset['text'].tolist(), padding=True, truncation=True, return_tensors="pt")

    # Prepare input tensors and labels
    input_ids = encoded_tweets['input_ids']
    attention_mask = encoded_tweets['attention_mask']
    labels = torch.tensor(dataset['sentiment'].tolist())

    # Split the dataset into training and validation sets
    train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.2, random_state=42)
    train_masks, val_masks, _, _ = train_test_split(attention_mask, labels, test_size=0.2, random_state=42)

    # Create DataLoader for training and validation sets
    train_dataset = TensorDataset(train_inputs, train_masks, train_labels)
    val_dataset = TensorDataset(val_inputs, val_masks, val_labels)

    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # Initialize the model
    model = TweetSentimentClassifier(pretrained_model_name, num_classes)

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-5)

    # Training loop
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for batch in train_dataloader:
            inputs, masks, labels = batch
            optimizer.zero_grad()
            outputs = model(inputs, attention_mask=masks)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_dataloader)
        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')

    # Save the trained model
    torch.save(model.state_dict(), 'sentiment_model.pth')

    print('Training complete. Model saved as sentiment_model.pth')

# Assuming you have a pandas DataFrame 'df' with 'text' and 'sentiment' columns
# where 'sentiment' is 0 for Negative, 1 for Neutral, and 2 for Positive
train_sentiment_model(df)

#call the data provided by charlie
#use the data to train the model/evaluate
#partition data into 80/20 80- is training; 20- is
#see how to preprocess the data

#instert trained model

#insert more of test model
    encoded_tweets = tokenizer(relevant_tweet_texts, padding=True, truncation=True, return_tensors="pt")
    predicted_logits = model(encoded_tweets['input_ids'], attention_mask=encoded_tweets['attention_mask'])
    predicted_labels = torch.argmax(predicted_logits, dim=1).tolist()

# Twitter API credentials (need to be done!!!)
consumer_key = "DPyiTMdzrgs2Q9C8VrHeFFA0c"
consumer_secret = "1iJ62QXgwFaXDUYhPQKMfHlwCvcUPE7Wa5CPRpXwjhxxJtMWDM"
access_token = "1721929948208742400-aS5vDw8vxUpnSp7uP60BlqUR17eMkU"
access_token_secret = "GsoEGY950R8PZwgH0djPHwIR3QCmxA4sBZMnY5YuSytDE"

# Authenticate with Twitter API
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth, wait_on_rate_limit=True)

# Define the time range
end_date = datetime(2023, 11, 28)

# Define sentiment labels
sentiment_labels = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}

# Define generalized political keywords
political_keywords = [
    'Climate Control','Healthcare','Gun Control'
]

# Specify multiple locations (East Coast states)
east_coast_states = {
    'Virginia':'37.4316,78.6569,215mi', 'North Carolina':'35.7596,79.0193,250mi', 'Maryland':'39.0458,76.6413,125mi', 'New York':'40.7128,74.0060,175mi', 'Florida':'27.6648,81.5158,223mi'
}

# Initialize a dictionary to store results for each state
results_by_state = {state: [] for state in east_coast_states}

# Loop through each state
for state in east_coast_states:
    # Initialize variables to store retrieved tweets
    all_tweets = []

    # Fetch tweets in batches with location filter and generalized political keywords
    batch_size = 300  # Adjust the batch size as needed
    location_query = f"{state} USA"
    cursor = tweepy.Cursor(api.search_tweets, q=" ".join(political_keywords), until=end_date, lang="en", geocode=east_coast_states[state]).items(batch_size)
    for tweet in cursor:
        tweet_data = {
            "text": tweet.text,
            "timestamp": tweet.created_at
        }
        all_tweets.append(tweet_data)

    # Filter and process tweets within the time range
    relevant_tweets = [
        tweet for tweet in all_tweets
        if start_date <= tweet['timestamp'] <= end_date
    ]
    #f = open("file.txt", "r")
    #text = f.read()

    # Extract the tweet text from the filtered tweets
    relevant_tweet_texts = [tweet['text'] for tweet in relevant_tweets]
    pickle.dump(relevant_tweet_texts, open ("relevanttwitterprojtweets.pkl", "wb"))
    pickle_load = pickle.load(open("relevanttwitterprojtweets.pkl","rb"))



# Tokenize and encode relevant tweets using the same tokenizer
    pretrained_model_name = 'vinai/bertweet-base'
    tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

    # Create the sentiment analysis model
    num_classes = 3  # 3 classes: Positive, Negative, Neutral
    model = TweetSentimentClassifier(pretrained_model_name, num_classes)


#training of the model

#change 'relevant_tweet_texts' for the new data set
    # Perform sentiment analysis
    encoded_tweets = tokenizer(relevant_tweet_texts, padding=True, truncation=True, return_tensors="pt")
    predicted_logits = model(encoded_tweets['input_ids'], attention_mask=encoded_tweets['attention_mask'])
    predicted_labels = torch.argmax(predicted_logits, dim=1).tolist()

    # Add sentiment labels and keywords to relevant tweets
    sentiments = [sentiment_labels[label] for label in predicted_labels]
    for i, tweet in enumerate(relevant_tweets):
        tweet['sentiment'] = sentiments[i]
        tweet['keyword'] = next((keyword for keyword in political_keywords if keyword in tweet['text'].lower()), 'Not Found')

    # Add results for the state to the dictionary
    results_by_state[state] = relevant_tweets

# Print or further process the results for each state
for state, results in results_by_state.items():
    print(f"Results for {state}:")
    for result in results:
        print(f"Text: {result['text']}\nKeyword: {result['keyword']}\nSentiment: {result['sentiment']}\n---")

# Calculate and print total percentages of each sentiment towards each keyword
keyword_sentiment_counts = {keyword: {'positive': 0, 'neutral': 0, 'negative': 0} for keyword in political_keywords}

for state, results in results_by_state.items():
    state_tweet_count = len(results)
    print(f"\nTotal Percentages of Each Sentiment Towards Each Keyword in {state}:")
    for keyword, counts in keyword_sentiment_counts.items():
        keyword_counts = {'positive': 0, 'neutral': 0, 'negative': 0}
        for result in results:
            if result['keyword'] == keyword:
                keyword_counts[result['sentiment'].lower()] += 1
        if state_tweet_count > 0:
            positive_percentage = (keyword_counts['positive'] / state_tweet_count) * 100
            neutral_percentage = (keyword_counts['neutral'] / state_tweet_count) * 100
            negative_percentage = (keyword_counts['negative'] / state_tweet_count) * 100
            print(f"Keyword: {keyword}")
            print(f"Positive: {positive_percentage:.2f}%")
            print(f"Neutral: {neutral_percentage:.2f}%")
            print(f"Negative: {negative_percentage:.2f}%")
            print("---")