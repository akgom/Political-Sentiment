# -*- coding: utf-8 -*-
"""temp_methods.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1995Xy0WLMkIVYQmHBjrPMdesnx0qr0Zo
"""

#MODEL
import torch
import torch.nn as nn
from transformers import BertModel

class TweetSentimentClassifier(nn.Module):
    def __init__(self, pretrained_model_name, num_classes):
        super(TweetSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.dropout = nn.Dropout(0.1)
        self.fc1 = nn.Linear(self.bert.config.hidden_size, 512)  # Add an additional linear layer
        self.fc2 = nn.Linear(512, num_classes)  # Output layer


    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = outputs['last_hidden_state'][:, 0, :]
         # Take the [CLS] token
        #output = self.dropout(pooled_output)
        output = torch.relu(self.fc1(pooled_output))  # Apply ReLU activation
        output = self.fc2(output)
        return output

from google.colab import drive
drive.mount('/content/drive')

# Ensure the necessary libraries are imported
import pandas as pd
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

# Load sentiment data
sentiment_data_path = '/content/drive/MyDrive/trainingdata-all-annotations.txt'  # Adjust the path accordingly
sentiment_df = pd.read_csv(sentiment_data_path, sep='\t', encoding='latin1')

# Filter columns of interest
relevant_columns = ['ID', 'Tweet', 'Sentiment']
cleaned_data = sentiment_df[relevant_columns]

# Tokenize tweets using BERT tokenizer
pretrained_model_name = 'vinai/bertweet-base'
tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

# Tokenize and encode tweets
encoded_tweets = tokenizer(cleaned_data['Tweet'].tolist(), padding=True, truncation=True, return_tensors="pt")

# Add encoded tweet IDs and attention masks to the cleaned_data DataFrame
cleaned_data['input_ids'] = encoded_tweets['input_ids'].tolist()
cleaned_data['attention_mask'] = encoded_tweets['attention_mask'].tolist()

# Determine class distribution for balancing
class_counts = cleaned_data['Sentiment'].value_counts()

# Perform oversampling and undersampling
oversample_neither = RandomOverSampler(sampling_strategy={'NEITHER': class_counts['POSITIVE'], 'POSITIVE': class_counts['POSITIVE'], 'NEGATIVE': class_counts['NEGATIVE']})
undersample_negative = RandomUnderSampler(sampling_strategy={'NEITHER': class_counts['NEITHER'], 'POSITIVE': class_counts['POSITIVE'], 'NEGATIVE': class_counts['POSITIVE']})

X = cleaned_data[['input_ids', 'attention_mask']]
y = cleaned_data['Sentiment']

X_neither_resampled, y_neither_resampled = oversample_neither.fit_resample(X, y)
X_negative_resampled, y_negative_resampled = undersample_negative.fit_resample(X, y)

# Combine resampled data into a DataFrame
resampled_data = pd.concat([X_neither_resampled, y_neither_resampled], axis=1)

# Split the data into train and dev sets
train_data, dev_data = train_test_split(resampled_data, test_size=0.2, random_state=42)

# Print information
print(train_data['Sentiment'].value_counts())
print(f"Positive: {class_counts['POSITIVE']}")
print(f"Negative: {class_counts['NEGATIVE']}")
print(f"Neither (after resampling): {len(y_neither_resampled)}")
print(f"Negative (after resampling): {len(y_negative_resampled)}")

# Save the cleaned data if needed
# cleaned_data.to_csv('/content/drive/MyDrive/cleaned_data.csv', index=False)
# train_data.to_csv('/content/drive/MyDrive/train_data.csv', index=False)
# dev_data.to_csv('/content/drive/MyDrive/dev_data.csv', index=False)

# Ensure necessary libraries are imported
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Load train_data and dev_data (assuming they are properly prepared)
train_data = ...
dev_data = ...

# Define label mapping function
def mapper(label):
    if label == "POSITIVE":
        return 0
    elif label == "NEGATIVE":
        return 1
    elif label == "NEITHER":
        return 2

# Map sentiment labels to numerical values
train_data['Sentiment'] = train_data['Sentiment'].map(lambda x: mapper(x))
dev_data['Sentiment'] = dev_data['Sentiment'].map(lambda x: mapper(x))

# Convert data to PyTorch tensors
train_input_ids = torch.tensor(train_data['input_ids'].tolist(), device=device)
train_attention_masks = torch.tensor(train_data['attention_mask'].tolist(), device=device)
train_labels = torch.tensor(train_data['Sentiment'].tolist(), device=device)

dev_input_ids = torch.tensor(dev_data['input_ids'].tolist(), device=device)
dev_attention_masks = torch.tensor(dev_data['attention_mask'].tolist(), device=device)
dev_labels = torch.tensor(dev_data['Sentiment'].tolist(), device=device)

# Create DataLoader for training and validation
batch_size = 16
train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

dev_dataset = TensorDataset(dev_input_ids, dev_attention_masks, dev_labels)
dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)

# Initialize the model
pretrained_model_name = 'vinai/bertweet-base'
num_classes = 3
model = TweetSentimentClassifier(pretrained_model_name, num_classes).to(device)

# Training settings
optimizer = optim.AdamW(model.parameters(), lr=5e-8)
criterion = nn.CrossEntropyLoss()

# Training loop
num_epochs = 100
model.train()
training_losses = []
dev_losses = []

for epoch in tqdm(range(num_epochs)):
    total_train_loss = 0
    total_dev_loss = 0

    for batch in train_dataloader:
        input_ids, attention_masks, labels = batch
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_masks)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    total_train_loss /= len(train_dataloader)
    training_losses.append(total_train_loss)

    model.eval()
    with torch.no_grad():
        for batch in dev_dataloader:
            input_ids, attention_masks, labels = batch
            outputs = model(input_ids, attention_mask=attention_masks)
            loss = criterion(outputs, labels)
            total_dev_loss += loss.item()

        total_dev_loss /= len(dev_dataloader)
        dev_losses.append(total_dev_loss)

# Save the trained model
torch.save(model.state_dict(), 'trained_model.pth')

# Save losses for each epoch
losses_data = pd.DataFrame({'Training Loss': training_losses, 'Dev Loss': dev_losses})
losses_data.to_csv('losses_data.csv', index=False)

# Plot the training and development losses
plt.plot(training_losses, label='Training Loss')
plt.plot(dev_losses, label='Dev Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Find the index of the smallest development loss
index_of_min_loss = np.argmin(dev_losses)
print(f"Index of the smallest development loss: {index_of_min_loss}")
print(f"Smallest development loss: {dev_losses[index_of_min_loss]}")

#TEST
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
from sklearn.metrics import accuracy_score, classification_report

# Load sentiment data
sentiment_data_path = '../testdata-taskB-all-annotations.txt'  # Replace with the actual path
sentiment_df = pd.read_csv(sentiment_data_path, sep='\t', encoding='latin1')

# Filter relevant columns
relevant_columns = ['ID', 'Tweet', 'Sentiment']
test_data = sentiment_df[relevant_columns]

# Tokenize tweets using BERT tokenizer
encoded_tweets = tokenizer(test_data['Tweet'].tolist(), padding=True, truncation=True, return_tensors="pt")

# Add the encoded tweet IDs and attention masks to the test_data DataFrame
test_data['input_ids'] = encoded_tweets['input_ids'].tolist()
test_data['attention_mask'] = encoded_tweets['attention_mask'].tolist()

# Map sentiment labels to numerical values
test_data['Sentiment'] = test_data['Sentiment'].map(lambda x: mapper(x))

# Convert test data to PyTorch tensors
test_input_ids = torch.tensor(test_data['input_ids'].tolist(), device=device)
test_attention_masks = torch.tensor(test_data['attention_mask'].tolist(), device=device)
test_labels = torch.tensor(test_data['Sentiment'].tolist(), device=device)
test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Initialize the model
model = TweetSentimentClassifier(pretrained_model_name, num_classes).to(device)

# Load the trained model weights
model.load_state_dict(torch.load("trained_model.pth"))  # Replace with the path to your trained model

# Testing loop
model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_masks, labels = batch
        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)

        outputs = model(input_ids, attention_mask=attention_masks)
        preds = F.softmax(outputs, dim=1)
        _, predicted_labels = torch.max(preds, 1)

        all_preds.extend(predicted_labels.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate accuracy and print classification report
accuracy = accuracy_score(all_labels, all_preds)
print(f'Accuracy: {accuracy:.4f}')

print('\nClassification Report:')
print(classification_report(all_labels, all_preds, target_names=["POSITIVE", "NEGATIVE", "NEITHER"]))

import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
import pickle
from transformers import BertTokenizer

# Load the data from the pickle file
relevant_data = pickle.load(open('relevant_data.pickle', 'rb'))

# Tokenize the text data using the same tokenizer used for training data
pretrained_model_name = 'vinai/bertweet-base'
tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

# Tokenize and encode the text data
encoded_tweets = tokenizer(relevant_data['text'].tolist(), padding=True, truncation=True, return_tensors="pt")

# Prepare input tensors
input_ids = encoded_tweets['input_ids']
attention_mask = encoded_tweets['attention_mask']

# Initialize the model
pretrained_model_name = 'vinai/bertweet-base'
num_classes = 3
model = TweetSentimentClassifier(pretrained_model_name, num_classes)

# Load the trained model weights
index_of_min_loss = 0  # Change this index to the epoch with the lowest dev loss
model.load_state_dict(torch.load("trained_model.pth"))  # Replace with correct path
model.eval()

# Convert data to PyTorch tensors
test_input_ids = torch.tensor(input_ids)
test_attention_masks = torch.tensor(attention_mask)

# Create DataLoader for testing
batch_size = 16
test_dataset = TensorDataset(test_input_ids, test_attention_masks)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Testing loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

all_preds = []

with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_masks = batch
        input_ids, attention_masks = input_ids.to(device), attention_masks.to(device)

        outputs = model(input_ids, attention_mask=attention_masks)
        preds = F.softmax(outputs, dim=1)
        _, predicted_labels = torch.max(preds, 1)

        all_preds.extend(predicted_labels.cpu().numpy())

# Process the outputs as required based on your application
# For example, save predictions to a file or perform further analysis