# -*- coding: utf-8 -*-
"""bert-base-cased method.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SbsTetxS6kdlAAqJi3022YFwYW4QPXF7
"""

import torch
import torch.nn as nn
from transformers import BertModel

class TweetSentimentClassifier(nn.Module):
    def __init__(self, pretrained_model_name, num_classes):
        super(TweetSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.dropout = nn.Dropout(0.1)
        self.fc1 = nn.Linear(self.bert.config.hidden_size, 512)  # Add an additional linear layer
        self.fc2 = nn.Linear(512, num_classes)  # Output layer


    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = outputs['last_hidden_state'][:, 0, :]
         # Take the [CLS] token
        #output = self.dropout(pooled_output)
        output = torch.relu(self.fc1(pooled_output))  # Apply ReLU activation
        output = self.fc2(output)
        return output

from google.colab import drive
drive.mount('/content/drive')

# Load sentiment data
import pandas as pd
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

sentiment_data_path = '../trainingdata-all-annotations.txt'  # Replace with the actual path
sentiment_df = pd.read_csv(sentiment_data_path, sep='\t', encoding='latin1')

relevant_columns = ['ID', 'Tweet', 'Sentiment']
cleaned_data = sentiment_df[relevant_columns]

pretrained_model_name = 'bert-base-cased'  # Change to BERT base cased
tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

positive_counter = 0
negative_counter = 0
neither_counter = 0

for x in cleaned_data["Sentiment"]:
    if x == "POSITIVE":
        positive_counter += 1
    elif x == "NEGATIVE":
        negative_counter += 1
    elif x == "NEITHER":
        neither_counter += 1

encoded_tweets = tokenizer(cleaned_data['Tweet'].tolist(), padding=True, truncation=True, return_tensors="pt")

cleaned_data['input_ids'] = encoded_tweets['input_ids'].tolist()
cleaned_data['attention_mask'] = encoded_tweets['attention_mask'].tolist()

X = cleaned_data[['input_ids', 'attention_mask']]
y = cleaned_data['Sentiment']

oversample_neither = RandomOverSampler(sampling_strategy={'NEITHER': positive_counter, 'POSITIVE': positive_counter, 'NEGATIVE': negative_counter})
undersample_negative = RandomUnderSampler(sampling_strategy={'NEITHER': neither_counter, 'POSITIVE': positive_counter, 'NEGATIVE': positive_counter})

X_neither_resampled, y_neither_resampled = oversample_neither.fit_resample(X, y)
X_negative_resampled, y_negative_resampled = undersample_negative.fit_resample(X, y)

resampled_data = pd.concat([X_neither_resampled, y_neither_resampled], axis=1)

train_data, dev_data = train_test_split(resampled_data, test_size=0.2, random_state=42)

print(f"Positive: {positive_counter}")
print(f"Negative: {negative_counter}")
print(f"Neither (after resampling): {len(y_neither_resampled)}")
print(f"Negative (after resampling): {len(y_negative_resampled)}")

# METHOD TRAINING
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Assuming you have already defined TweetSentimentClassifier class

# Load cleaned data (Assuming cleaned_data is already available)
# cleaned_data_path = 'path/to/cleaned_data.csv'  # Replace with the actual path
# cleaned_data = pd.read_csv(cleaned_data_path)

# Map sentiment labels to numerical values
def mapper(label):
    if label == "POSITIVE":
        return 0
    elif label == "NEGATIVE":
        return 1
    elif label == "NEITHER":
        return 2

# Assuming train_data and test_data are already defined
train_data['Sentiment'] = train_data['Sentiment'].map(lambda x: mapper(x))
dev_data['Sentiment'] = dev_data['Sentiment'].map(lambda x: mapper(x))

# Split the data into train and test sets

train_input_ids = torch.tensor(train_data['input_ids'].tolist(), device=device)
train_attention_masks = torch.tensor(train_data['attention_mask'].tolist(), device=device)
train_labels = torch.tensor(train_data['Sentiment'].tolist(), device=device)

dev_input_ids = torch.tensor(dev_data['input_ids'].tolist(), device=device)
dev_attention_masks = torch.tensor(dev_data['attention_mask'].tolist(), device=device)
dev_labels = torch.tensor(dev_data['Sentiment'].tolist(), device=device)

# Create DataLoader for training and testing
batch_size = 16
train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

dev_dataset = TensorDataset(dev_input_ids, dev_attention_masks, dev_labels)
dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)

# Initialize the model
pretrained_model_name = 'vinai/bertweet-base'
num_classes = 3
model = TweetSentimentClassifier(pretrained_model_name, num_classes).to(device)

# Training settings
optimizer = optim.AdamW(model.parameters(), lr=5e-8) #change to a lower number if the graph is too high and spiked

class_counts = torch.bincount(train_labels)
total_samples = len(train_labels)
class_weights = total_samples / (len(class_counts) * class_counts.float())
class_weights /= class_weights.sum()

print(class_weights)

#class_weights = torch.tensor([.2375, .0125, .75], device=device)#use iphone formula
criterion = nn.CrossEntropyLoss(weight=class_weights)
#criterion = nn.CrossEntropyLoss()

# Training loop
num_epochs = 10  # switch to 100 when the code works (to test)
model.to(device)

# Lists to store training and development losses
training_losses = []
dev_losses = []

for epoch in tqdm(range(num_epochs)):
    model.train()
    total_train_loss = 0
    total_dev_loss = 0

    for batch in train_dataloader:
        input_ids, attention_masks, labels = batch

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_masks)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()
    total_train_loss = total_train_loss / len(train_dataloader)

    torch.save(model.state_dict(), 'epoch_' + str(epoch) + '.pth')
    model.eval()
    with torch.no_grad():

        for batch in dev_dataloader:
            input_ids, attention_masks, labels = batch
            outputs = model(input_ids, attention_mask=attention_masks)
            loss = criterion(outputs, labels)
            total_dev_loss += loss.item()
        total_dev_loss = total_dev_loss / len(dev_dataloader)

        training_losses.append(total_train_loss)
        dev_losses.append(total_dev_loss)

        print(dev_losses)  # can compare this to the output to check accuracy

# Save the trained model
torch.save(model.state_dict(), 'trained_model.pth')

# Save losses for each epoch on the dev + train sets
losses_data = pd.DataFrame({'Training Loss': training_losses, 'Dev Loss': dev_losses})
losses_data.to_csv('losses_data.csv', index=False)

# Plot the training and development losses
plt.plot(training_losses, label='Training Loss')
plt.plot(dev_losses, label='Dev Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Find the index of the smallest development loss
index_of_min_loss = np.argmin(dev_losses)
print(f"Index of the smallest development loss: {index_of_min_loss}")
print(f"Smallest development loss: {dev_losses[index_of_min_loss]}")

#TEST
import torch
from torch.utils.data import DataLoader
import torch.nn.functional as F
from sklearn.metrics import accuracy_score, classification_report
#load the model to run

sentiment_data_path = '../testdata-taskB-all-annotations.txt' # Replace with the actual path
sentiment_df = pd.read_csv(sentiment_data_path, sep='\t', encoding = 'latin1')

# Load tweet ID data

# Merge sentiment and tweet ID data on 'ID'
#merged_df = pd.merge(sentiment_df, tweet_id_df, on='ID')

# Filter columns of interest
relevant_columns = ['ID', 'Tweet', 'Sentiment']
test_data = sentiment_df[relevant_columns]

# Assuming you have 'Tweet' column in the cleaned_data DataFrame
encoded_tweets = tokenizer(test_data['Tweet'].tolist(), padding=True, truncation=True, return_tensors="pt")

# Add the encoded tweet IDs and attention masks to the cleaned_data DataFrame
test_data['input_ids'] = encoded_tweets['input_ids'].tolist()
test_data['attention_mask'] = encoded_tweets['attention_mask'].tolist()




# Convert test data to PyTorch tensors
test_data['Sentiment'] = test_data['Sentiment'].map(lambda x: mapper(x))

test_input_ids = torch.tensor(test_data['input_ids'].tolist(), device = device)
test_attention_masks = torch.tensor(test_data['attention_mask'].tolist(), device = device)
test_labels = torch.tensor(test_data['Sentiment'].tolist(), device = device)
test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)


# Initialize the model
pretrained_model_name = 'vinai/bertweet-base'
num_classes = 3
model = TweetSentimentClassifier(pretrained_model_name, num_classes)

# Load the trained model weights
model.load_state_dict(torch.load("epoch_" + str(index_of_min_loss) + ".pth"))#replace with saved model (should be in my computer)
model.eval()

# Testing loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_masks, labels = batch
        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)

        outputs = model(input_ids, attention_mask=attention_masks)
        preds = F.softmax(outputs, dim=1)
        _, predicted_labels = torch.max(preds, 1)

        all_preds.extend(predicted_labels.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate accuracy and print classification report
accuracy = accuracy_score(all_labels, all_preds)
print(f'Accuracy: {accuracy:.4f}')

print('\nClassification Report:')
print(classification_report(all_labels, all_preds, target_names=list(["POSITIVE", "NEGATIVE", "NEITHER"])))