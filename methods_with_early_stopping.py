# -*- coding: utf-8 -*-
"""methods_with early stopping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12XW4To9Il4fPJJsb5v3DV_mRA5IY1QdR
"""

#MODEL
import torch
import torch.nn as nn
from transformers import BertModel

class TweetSentimentClassifier(nn.Module):
    def __init__(self, pretrained_model_name, num_classes):
        super(TweetSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.dropout = nn.Dropout(0.1)
        self.fc1 = nn.Linear(self.bert.config.hidden_size, 512)  # Add an additional linear layer
        self.fc2 = nn.Linear(512, num_classes)  # Output layer


    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = outputs['last_hidden_state'][:, 0, :]
         # Take the [CLS] token
        #output = self.dropout(pooled_output)
        output = torch.relu(self.fc1(pooled_output))  # Apply ReLU activation
        output = self.fc2(output)
        return output

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

# Load sentiment data
sentiment_data_path = '../trainingdata-all-annotations.txt'
sentiment_df = pd.read_csv(sentiment_data_path, sep='\t', encoding = 'latin1')

# Load twitter ID data

# Merge sentiment and tweet ID data on 'ID'

# Filter columns of interest
relevant_columns = ['ID', 'Tweet', 'Sentiment']
cleaned_data = sentiment_df[relevant_columns]

# Tokenize tweets using BERT tokenizer
pretrained_model_name = 'vinai/bertweet-base'
tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

positive_counter = 0;
negative_counter = 0;
neither_counter = 0;

for x in cleaned_data["Sentiment"]:
  if x == "POSITIVE":
      positive_counter += 1
  elif x == "NEGATIVE":
      negative_counter +=1
  elif x == "NEITHER":
      neither_counter +=1

# Assuming you have 'Tweet' column in the cleaned_data DataFrame
encoded_tweets = tokenizer(cleaned_data['Tweet'].tolist(), padding=True, truncation=True, return_tensors="pt")

# Add the encoded tweet IDs and attention masks to the cleaned_data DataFrame
cleaned_data['input_ids'] = encoded_tweets['input_ids'].tolist()
cleaned_data['attention_mask'] = encoded_tweets['attention_mask'].tolist()

X = cleaned_data[['input_ids', 'attention_mask']]
y = cleaned_data['Sentiment']

# Split the data into train and test sets REMOVE THIS AND BELOW FROM FUNC.
oversample_neither = RandomOverSampler(sampling_strategy={'NEITHER': positive_counter, 'POSITIVE': positive_counter, 'NEGATIVE': negative_counter})
undersample_negative = RandomUnderSampler(sampling_strategy={'NEITHER': neither_counter, 'POSITIVE': positive_counter, 'NEGATIVE': positive_counter})

X_neither_resampled, y_neither_resampled = oversample_neither.fit_resample(X, y)
X_negative_resampled, y_negative_resampled = undersample_negative.fit_resample(X, y)

resampled_data = pd.concat([X_neither_resampled, y_neither_resampled], axis=1)

train_data, dev_data = train_test_split(resampled_data, test_size=0.2, random_state=42)

#include oversampling with neither + undersampling with negative

#split to dev data
print(train_data['Sentiment'])

print(f"Positive: {positive_counter}")
print(f"Negative: {negative_counter}")
print(f"Neither (after resampling): {len(y_neither_resampled)}")
print(f"Negative (after resampling): {len(y_negative_resampled)}")


# Save the cleaned data (if needed)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load sentiment data
sentiment_data_path = '../trainingdata-all-annotations.txt'
sentiment_df = pd.read_csv(sentiment_data_path, sep='\t', encoding='latin1')

# Filter columns of interest
relevant_columns = ['Tweet', 'Sentiment']
cleaned_data = sentiment_df[relevant_columns]

# Tokenize tweets using BERT tokenizer
pretrained_model_name = 'vinai/bertweet-base'
tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

# Encode tweets
encoded_tweets = tokenizer(cleaned_data['Tweet'].tolist(), padding=True, truncation=True, return_tensors="pt")

# Add the encoded tweet IDs and attention masks to the cleaned_data DataFrame
cleaned_data['input_ids'] = encoded_tweets['input_ids'].tolist()
cleaned_data['attention_mask'] = encoded_tweets['attention_mask'].tolist()

# Map sentiment labels to numerical values
def mapper(label):
    if label == "POSITIVE":
        return 0
    elif label == "NEGATIVE":
        return 1
    elif label == "NEITHER":
        return 2

cleaned_data['Sentiment'] = cleaned_data['Sentiment'].map(lambda x: mapper(x))

# Split data into train and dev sets
train_data, dev_data = train_test_split(cleaned_data, test_size=0.2, random_state=42)

# Create DataLoader for training and dev sets
batch_size = 16
train_input_ids = torch.tensor(train_data['input_ids'].tolist(), device=device)
train_attention_masks = torch.tensor(train_data['attention_mask'].tolist(), device=device)
train_labels = torch.tensor(train_data['Sentiment'].tolist(), device=device)

train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

dev_input_ids = torch.tensor(dev_data['input_ids'].tolist(), device=device)
dev_attention_masks = torch.tensor(dev_data['attention_mask'].tolist(), device=device)
dev_labels = torch.tensor(dev_data['Sentiment'].tolist(), device=device)

dev_dataset = TensorDataset(dev_input_ids, dev_attention_masks, dev_labels)
dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)

# Initialize the model
class TweetSentimentClassifier(nn.Module):
    def __init__(self, pretrained_model_name, num_classes):
        super(TweetSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.dropout = nn.Dropout(0.1)
        self.fc1 = nn.Linear(self.bert.config.hidden_size, 512)  # Add an additional linear layer
        self.fc2 = nn.Linear(512, num_classes)  # Output layer

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = outputs['last_hidden_state'][:, 0, :]
        output = torch.relu(self.fc1(pooled_output))  # Apply ReLU activation
        output = self.fc2(output)
        return output

pretrained_model_name = 'vinai/bertweet-base'
num_classes = 3
model = TweetSentimentClassifier(pretrained_model_name, num_classes).to(device)

# Training settings
optimizer = optim.AdamW(model.parameters(), lr=5e-8)
criterion = nn.CrossEntropyLoss()

# Initialize early stopping parameters
best_dev_loss = float('inf')
epochs_without_improvement = 0
early_stopping_patience = 10

# Training loop
num_epochs = 1000
training_losses = []
dev_losses = []

for epoch in tqdm(range(num_epochs)):
    model.train()
    total_train_loss = 0

    for batch in train_dataloader:
        input_ids, attention_masks, labels = batch

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_masks)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    total_train_loss /= len(train_dataloader)
    training_losses.append(total_train_loss)

    # Evaluate on development set
    model.eval()
    total_dev_loss = 0
    with torch.no_grad():
        for batch in dev_dataloader:
            input_ids, attention_masks, labels = batch
            outputs = model(input_ids, attention_mask=attention_masks)
            loss = criterion(outputs, labels)
            total_dev_loss += loss.item()

    total_dev_loss /= len(dev_dataloader)
    dev_losses.append(total_dev_loss)

    # Check for early stopping
    if total_dev_loss < best_dev_loss:
        best_dev_loss = total_dev_loss
        epochs_without_improvement = 0
        torch.save(model.state_dict(), 'best_model.pth')
    else:
        epochs_without_improvement += 1
        if epochs_without_improvement >= early_stopping_patience:
            print(f'Early stopping at epoch {epoch}')
            break

# Save the trained model
torch.save(model.state_dict(), 'trained_model.pth')

# Save losses for each epoch on the dev + train sets
losses_data = pd.DataFrame({'Training Loss': training_losses, 'Dev Loss': dev_losses})
losses_data.to_csv('losses_data.csv', index=False)

# Plot the training and development losses
plt.plot(training_losses, label='Training Loss')
plt.plot(dev_losses, label='Dev Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Find the index of the smallest development loss
index_of_min_loss = np.argmin(dev_losses)
print(f"Index of the smallest development loss: {index_of_min_loss}")
print(f"Smallest development loss: {dev_losses[index_of_min_loss]}")

#TEST
import torch
from torch.utils.data import DataLoader
import torch.nn.functional as F
from sklearn.metrics import accuracy_score, classification_report
#load the model to run

sentiment_data_path = '../testdata-taskB-all-annotations.txt' # Replace with the actual path
sentiment_df = pd.read_csv(sentiment_data_path, sep='\t', encoding = 'latin1')

# Load tweet ID data

# Merge sentiment and tweet ID data on 'ID'
#merged_df = pd.merge(sentiment_df, tweet_id_df, on='ID')

# Filter columns of interest
relevant_columns = ['ID', 'Tweet', 'Sentiment']
test_data = sentiment_df[relevant_columns]

# Assuming you have 'Tweet' column in the cleaned_data DataFrame
encoded_tweets = tokenizer(test_data['Tweet'].tolist(), padding=True, truncation=True, return_tensors="pt")

# Add the encoded tweet IDs and attention masks to the cleaned_data DataFrame
test_data['input_ids'] = encoded_tweets['input_ids'].tolist()
test_data['attention_mask'] = encoded_tweets['attention_mask'].tolist()




# Convert test data to PyTorch tensors
test_data['Sentiment'] = test_data['Sentiment'].map(lambda x: mapper(x))

test_input_ids = torch.tensor(test_data['input_ids'].tolist(), device = device)
test_attention_masks = torch.tensor(test_data['attention_mask'].tolist(), device = device)
test_labels = torch.tensor(test_data['Sentiment'].tolist(), device = device)
test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)


# Initialize the model
pretrained_model_name = 'vinai/bertweet-base'
num_classes = 3
model = TweetSentimentClassifier(pretrained_model_name, num_classes)

# Load the trained model weights
model.load_state_dict(torch.load("epoch_" + str(index_of_min_loss) + ".pth"))#replace with saved model (should be in my computer)
model.eval()

# Testing loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_masks, labels = batch
        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)

        outputs = model(input_ids, attention_mask=attention_masks)
        preds = F.softmax(outputs, dim=1)
        _, predicted_labels = torch.max(preds, 1)

        all_preds.extend(predicted_labels.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate accuracy and print classification report
accuracy = accuracy_score(all_labels, all_preds)
print(f'Accuracy: {accuracy:.4f}')

print('\nClassification Report:')
print(classification_report(all_labels, all_preds, target_names=list(["POSITIVE", "NEGATIVE", "NEITHER"])))

import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
import pickle
from transformers import BertTokenizer
import matplotlib.pyplot as plt

# Load the data from the pickle file
relevant_data = pickle.load(open('/relevant_data(test).pickle', 'rb'))

# Tokenize the text data using the same tokenizer used for training data
pretrained_model_name = 'vinai/bertweet-base'
tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

# Initialize counters for sentiment analysis
keyword_sentiments = {keyword: {'positive': 0, 'negative': 0, 'neutral': 0} for keyword in keywords}

# Tokenize and encode the text data
for data in relevant_data:
    text = data['text']
    keyword = None
    for kw in keywords:
        if kw.lower() in text.lower():
            keyword = kw
            break
    if keyword is None:
        continue  # Skip if no keyword found
    encoded_tweet = tokenizer(text, padding=True, truncation=True, return_tensors="pt")
    input_ids = encoded_tweet['input_ids']
    attention_mask = encoded_tweet['attention_mask']

    # Prepare input tensors
    input_ids = input_ids.to(device)
    attention_mask = attention_mask.to(device)

    # Perform sentiment analysis
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        preds = F.softmax(outputs, dim=1)
        _, predicted_label = torch.max(preds, 1)

        # Update sentiment counts based on predicted label
        if predicted_label == 0:
            keyword_sentiments[keyword]['negative'] += 1
        elif predicted_label == 1:
            keyword_sentiments[keyword]['neutral'] += 1
        elif predicted_label == 2:
            keyword_sentiments[keyword]['positive'] += 1

# Plotting the results
for keyword, sentiments in keyword_sentiments.items():
    labels = list(sentiments.keys())
    counts = list(sentiments.values())

    plt.figure(figsize=(8, 6))
    plt.bar(labels, counts, color=['red', 'grey', 'green'])
    plt.title(f'Sentiment Analysis for Keyword: {keyword}')
    plt.xlabel('Sentiment')
    plt.ylabel('Count')
    plt.show()