# -*- coding: utf-8 -*-
"""Copy of methods_and_such.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RDyc0eNTNwnQ4kFg453eTZ-6AmnBv7Fj
"""

# Import necessary libraries
import torch
import torch.nn as nn
from transformers import BertModel

# Model definition
class TweetSentimentClassifier(nn.Module):
    def __init__(self, pretrained_model_name, num_classes):
        super(TweetSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_model_name)
        self.dropout = nn.Dropout(0.1)
        self.fc1 = nn.Linear(self.bert.config.hidden_size, 512)  # Add an additional linear layer
        self.fc2 = nn.Linear(512, num_classes)  # Output layer

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = outputs['last_hidden_state'][:, 0, :]
        output = torch.relu(self.fc1(pooled_output))  # Apply ReLU activation
        output = self.fc2(output)
        return output

# The rest of the code (data preprocessing, training loop, etc.) remains the same
# Implement early stopping in the training loop as shown in the previous response.

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

# Load sentiment data
sentiment_data_path = '../trainingdata-all-annotations.txt'
sentiment_df = pd.read_csv(sentiment_data_path, sep='\t', encoding = 'latin1')

# Load twitter ID data

# Merge sentiment and tweet ID data on 'ID'

# Filter columns of interest
relevant_columns = ['ID', 'Tweet', 'Sentiment']
cleaned_data = sentiment_df[relevant_columns]

# Tokenize tweets using BERT tokenizer
pretrained_model_name = 'vinai/bertweet-base'
tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

positive_counter = 0;
negative_counter = 0;
neither_counter = 0;

for x in cleaned_data["Sentiment"]:
  if x == "POSITIVE":
      positive_counter += 1
  elif x == "NEGATIVE":
      negative_counter +=1
  elif x == "NEITHER":
      neither_counter +=1

# Assuming you have 'Tweet' column in the cleaned_data DataFrame
encoded_tweets = tokenizer(cleaned_data['Tweet'].tolist(), padding=True, truncation=True, return_tensors="pt")

# Add the encoded tweet IDs and attention masks to the cleaned_data DataFrame
cleaned_data['input_ids'] = encoded_tweets['input_ids'].tolist()
cleaned_data['attention_mask'] = encoded_tweets['attention_mask'].tolist()

X = cleaned_data[['input_ids', 'attention_mask']]
y = cleaned_data['Sentiment']

# Split the data into train and test sets REMOVE THIS AND BELOW FROM FUNC.
oversample_neither = RandomOverSampler(sampling_strategy={'NEITHER': positive_counter, 'POSITIVE': positive_counter, 'NEGATIVE': negative_counter})
undersample_negative = RandomUnderSampler(sampling_strategy={'NEITHER': neither_counter, 'POSITIVE': positive_counter, 'NEGATIVE': positive_counter})

X_neither_resampled, y_neither_resampled = oversample_neither.fit_resample(X, y)
X_negative_resampled, y_negative_resampled = undersample_negative.fit_resample(X, y)

resampled_data = pd.concat([X_neither_resampled, y_neither_resampled], axis=1)

train_data, dev_data = train_test_split(resampled_data, test_size=0.2, random_state=42)

#include oversampling with neither + undersampling with negative

#split to dev data
print(train_data['Sentiment'])

print(f"Positive: {positive_counter}")
print(f"Negative: {negative_counter}")
print(f"Neither (after resampling): {len(y_neither_resampled)}")
print(f"Negative (after resampling): {len(y_negative_resampled)}")


# Save the cleaned data (if needed)

import pandas as pd
import torch
from torch.utils.data import DataLoader, TensorDataset
from transformers import BertTokenizer
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from tqdm import tqdm

# Initialize early stopping parameters
best_dev_loss = float('inf')
epochs_without_improvement = 0
early_stopping_patience = 5  # Number of epochs to wait before early stopping

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def mapper(label):
    if label == "POSITIVE":
        return 0
    elif label == "NEGATIVE":
        return 1
    elif label == "NEITHER":
        return 2
    else:
        return -1  # Unknown label

# Assuming train_data and dev_data are already defined
# Assuming train_data and dev_data are already defined
train_data['Sentiment'] = train_data['Sentiment'].map(lambda x: mapper(x))
dev_data['Sentiment'] = dev_data['Sentiment'].map(lambda x: mapper(x))

# Drop rows with unknown labels
train_data_filtered = train_data[train_data['Sentiment'] != -1]
dev_data_filtered = dev_data[dev_data['Sentiment'] != -1]

# Check if there are samples remaining in the filtered training dataset
if len(train_data_filtered) == 0:
    raise ValueError("No samples remaining in the training dataset after filtering")

# Create train dataset and dataloader only if there are samples remaining
train_input_ids = torch.tensor(train_data_filtered['input_ids'].tolist(), device=device)
train_attention_masks = torch.tensor(train_data_filtered['attention_mask'].tolist(), device=device)
train_labels = torch.tensor(train_data_filtered['Sentiment'].tolist(), device=device)

# Create DataLoader for training and testing
batch_size = 16
train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Continue with the training loop...

# Assuming dev_data is already defined
dev_input_ids = torch.tensor(dev_data['input_ids'].tolist(), device=device)
dev_attention_masks = torch.tensor(dev_data['attention_mask'].tolist(), device=device)
dev_labels = torch.tensor(dev_data['Sentiment'].tolist(), device=device)
dev_dataset = TensorDataset(dev_input_ids, dev_attention_masks, dev_labels)
dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)

for epoch in tqdm(range(num_epochs)):
    model.train()
    total_train_loss = 0
    total_dev_loss = 0

    for batch in train_dataloader:
        input_ids, attention_masks, labels = batch

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_masks)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    total_train_loss = total_train_loss / len(train_dataloader)

    # Evaluate on development set
    model.eval()
    with torch.no_grad():
        for batch in dev_dataloader:
            input_ids, attention_masks, labels = batch
            outputs = model(input_ids, attention_mask=attention_masks)
            loss = criterion(outputs, labels)
            total_dev_loss += loss.item()
        total_dev_loss = total_dev_loss / len(dev_dataloader)

        # Check for early stopping
        if total_dev_loss < best_dev_loss:
            best_dev_loss = total_dev_loss
            epochs_without_improvement = 0
            # Save the model
            torch.save(model.state_dict(), 'best_model.pth')
        else:
            epochs_without_improvement += 1
            if epochs_without_improvement >= early_stopping_patience:
                print(f'Early stopping at epoch {epoch}')
                break

    # Record losses
    training_losses.append(total_train_loss)
    dev_losses.append(total_dev_loss)

    print(dev_losses)  # can compare this to the output to check accuracy

# Save the trained model
torch.save(model.state_dict(), 'trained_model.pth')

# Save losses for each epoch on the dev + train sets
losses_data = pd.DataFrame({'Training Loss': training_losses, 'Dev Loss': dev_losses})
losses_data.to_csv('losses_data.csv', index=False)

# Plot the training and development losses
plt.plot(training_losses, label='Training Loss')
plt.plot(dev_losses, label='Dev Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Find the index of the smallest development loss
index_of_min_loss = np.argmin(dev_losses)
print(f"Index of the smallest development loss: {index_of_min_loss}")
print(f"Smallest development loss: {dev_losses[index_of_min_loss]}")

# Initialize the model
pretrained_model_name = 'vinai/bertweet-base'
num_classes = 3
model = TweetSentimentClassifier(pretrained_model_name, num_classes)

# Load the best trained model weights
best_model_path = 'best_model.pth'  # Path to the best model saved during training
model.load_state_dict(torch.load(best_model_path))
model.eval()

# Move the model to the appropriate device
model.to(device)

# Testing loop
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_masks, labels = batch
        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)

        outputs = model(input_ids, attention_mask=attention_masks)
        preds = F.softmax(outputs, dim=1)
        _, predicted_labels = torch.max(preds, 1)

        all_preds.extend(predicted_labels.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Calculate accuracy and print classification report
accuracy = accuracy_score(all_labels, all_preds)
print(f'Accuracy: {accuracy:.4f}')

print('\nClassification Report:')
print(classification_report(all_labels, all_preds, target_names=list(["POSITIVE", "NEGATIVE", "NEITHER"])))

# Load the data from the pickle file
relevant_data = pickle.load(open('/relevant_data(test).pickle', 'rb'))

# Tokenize the text data using the same tokenizer used for training data
pretrained_model_name = 'vinai/bertweet-base'
tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)

# Tokenize and encode the text data
encoded_tweets = tokenizer(relevant_data['text'].tolist(), padding=True, truncation=True, return_tensors="pt")

# Prepare input tensors
input_ids = encoded_tweets['input_ids']
attention_mask = encoded_tweets['attention_mask']

# Initialize the model
pretrained_model_name = 'vinai/bertweet-base'
num_classes = 3
model = TweetSentimentClassifier(pretrained_model_name, num_classes)

# Load the best trained model weights
best_model_path = 'best_model.pth'  # Path to the best model saved during training
model.load_state_dict(torch.load(best_model_path))
model.eval()

# Convert data to PyTorch tensors
test_input_ids = torch.tensor(input_ids)
test_attention_masks = torch.tensor(attention_mask)

# Create DataLoader for testing
batch_size = 16
test_dataset = TensorDataset(test_input_ids, test_attention_masks)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Testing loop
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

all_preds = []

with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_masks = batch
        input_ids, attention_masks = input_ids.to(device), attention_masks.to(device)

        outputs = model(input_ids, attention_mask=attention_masks)
        preds = F.softmax(outputs, dim=1)
        _, predicted_labels = torch.max(preds, 1)

        all_preds.extend(predicted_labels.cpu().numpy())

# Process the outputs as required based on your application
# For example, save predictions to a file or perform further analysis